"""
Read list of files on clowder generated by rename.py and analyze for duplicates.

https://clowder.ncsa.illinois.edu/swagger/?url=https://clowder.ncsa.illinois.edu/clowder/swagger#/
"""
import json
import shelve
import sys
from collections import defaultdict, namedtuple

import requests
from clowder import ClowderRemote

Row = namedtuple("Row", "name newname size created id")


def get_files_from_log() -> (dict, int, int):
    """Reads log file generated by 'rename.py'."""
    files = defaultdict(list)
    total = 0
    waste = 0
    for line in sys.stdin:
        if line.startswith("#"):
            continue
        line = Row._make(eval(line))
        files[(line.name, line.size)].append(line)
    for name, size in files.keys():
        total += len(files[(name, size)]) * size
    files = {k: v for k, v in files.items() if len(v) != 1}
    for name, size in files.keys():
        print(name, size, len(files[(name, size)]))
        waste += (len(files[(name, size)]) - 1) * size
    return files, total, waste


def update_hashes(remote, hashes, file_ID):
    """Add hash for file_ID to hashes"""
    file_id = file_ID.id
    params = {"key": remote.path["key"]}

    url = f'{remote.path["host"]}' f"api/files/{file_id}/metadata.jsonld"
    response = requests.get(url, params=params)
    try:
        response.raise_for_status()
    except Exception:
        print(url)
        raise
    metas = response.json()
    if len(metas) == 0:
        print(f"Found NO metas: {file_id}")
    metas = [
        i
        for i in metas
        if i.get("agent", {}).get("name", "").endswith("ncsa.file.digest")
    ]
    hashes[file_ID.name].update(i["content"]["sha256"] for i in metas)
    print(".", end="")
    # print(file_ID.name, file_id, metas[0]["content"]["sha256"])


def de_dupe(file_ids: list[Row]) -> (str, list):
    """Check all hashes are the same and find a file_id with previews if one exists."""

    remote = ClowderRemote()

    hashes = defaultdict(set)
    delete = set()
    keep = None
    params = {"key": remote.path["key"]}

    for file_ID in file_ids:  # Collect hashes
        update_hashes(remote, hashes, file_ID)
        delete.add(file_ID.id)

    if not keep:
        for file_ID in file_ids:  # Check for previews
            file_id = file_ID.id

            url = f'{remote.path["host"]}' f"api/files/{file_id}/extracted_metadata"
            response = requests.get(url, params=params)
            try:
                response.raise_for_status()
            except Exception:
                print(url)
                raise
            metas = response.json()
            if len(metas) == 0:
                print(f"Found NO extracts: {file_id}")
            if metas.get("previews"):
                keep = file_id
                break

    assert len(hashes) == 1, hashes  # all file_ids for same name
    assert len(hashes[file_ID.name]) == 1  # only one hash for file name
    print()
    if keep:
        delete.discard(keep)
    else:
        keep = delete.pop()
    assert keep not in delete
    assert keep
    return keep, delete


files, total, waste = get_files_from_log()
print(f"{total:,}")
print(f"{waste:,}")

# check all files same length for name
assert len(files) == len(set(i[0] for i in files))

with shelve.open("done.pickle") as done:
    for (file_name, size), file_ids in files.items():
        print(file_name)
        if file_name not in done:
            # if not file_name.startswith("2023"):
            #     continue
            keep, delete = de_dupe(file_ids)
            done[file_name] = {"keep": keep, "delete": delete}
            print(keep, "/", len(delete))
            done.sync()
    data = dict(done.items())
    json.dump(data, open("done.json", "w"), indent=4, default=list)
